{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.io import arff\n",
    "from itertools import combinations\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_validate,StratifiedKFold, RandomizedSearchCV, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person = dict[str,str|pd.DataFrame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer_from_paper(labels, prediction):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fprs, tprs, _ = roc_curve(labels, prediction,pos_label=True)\n",
    "    eer = fprs[np.nanargmin(np.absolute((1 - tprs) - fprs))]\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer_changjiang(labels, prediction):\n",
    "    from scipy.optimize import brentq\n",
    "    from scipy.interpolate import interp1d\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(labels, prediction,pos_label=True)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eer(y_true, y_score):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=True)\n",
    "    abs_diff = [abs(fp-tp) for fp, tp in zip(fpr, tpr)]\n",
    "    min_idx = abs_diff.index(min(abs_diff))\n",
    "    eer = (fpr[min_idx] + tpr[min_idx])/2\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['ACTIVITY'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_dataframe(raw: pd.DataFrame, cols_to_drop: list[str]) -> pd.DataFrame:\n",
    "    result = raw.drop(columns=cols_to_drop)\n",
    "    result.columns = [col.replace('\"', \"\") for col in result]\n",
    "    result['ACTIVITY'] = result['ACTIVITY'].str.decode('utf-8')\n",
    "    result['class'] = result['class'].str.decode('utf-8')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_impostors(guid: str, people: list[dict[str, pd.DataFrame]]) -> list[dict[str, pd.DataFrame]]:\n",
    "    return random.choices([i for i in people if i['guid'] != guid], k=18)\n",
    "\n",
    "def biometric_train_test_split(person: Person, impostors: list[Person], activity: str,sensors: list[str])->[pd.DataFrame,pd.DataFrame]:\n",
    "    full_person: pd.DataFrame = None\n",
    "    for sensor in sensors:\n",
    "        full_person = person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True) if full_person is None else full_person.merge(\n",
    "                person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True), left_index=True, right_index=True)\n",
    "    full_person['class']=True\n",
    "\n",
    "    full_impostors: list[pd.DataFrame]|pd.DataFrame = []\n",
    "    for impostor in impostors:\n",
    "        full_impostor: pd.DataFrame = None\n",
    "        for sensor in sensors:\n",
    "            full_impostor = impostor[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True) if full_impostor is None else full_impostor.merge(\n",
    "                impostor[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True), left_index=True, right_index=True)\n",
    "        full_impostor = full_impostor.dropna()\n",
    "        if len(full_impostor)>=3:\n",
    "            full_impostor = full_impostor.sample(n=3)\n",
    "        full_impostor['class']=False\n",
    "        full_impostors.append(full_impostor)\n",
    "    full_person = full_person.sample(frac=1)\n",
    "    full_impostors = pd.concat(full_impostors).sample(frac=1)\n",
    "    return pd.concat([full_person[:len(full_person)//2],full_impostors[:len(full_impostors)//2]]),pd.concat([full_person[len(full_person)//2:],full_impostors[len(full_impostors)//2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_classification_df_builder(people: list[dict[str, int | pd.DataFrame]], sensors: list[str], activity: str) -> pd.DataFrame:\n",
    "    result_df: pd.DataFrame = None\n",
    "    for person in people:\n",
    "        df: pd.DataFrame = None\n",
    "        for sensor in sensors:\n",
    "            df = person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True) if df is None else df.merge(\n",
    "                person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True), left_index=True, right_index=True)\n",
    "            # df = pd.get_dummies(df)\n",
    "        df['class']=person['guid']\n",
    "        result_df = df if result_df is None else pd.concat([result_df, df])\n",
    "    return result_df.dropna()\n",
    "\n",
    "def activity_classification_df_builder(people: list[dict[str, int | pd.DataFrame]], sensors: list[str], activities: list[str]) -> pd.DataFrame:\n",
    "    people_df: list[pd.DataFrame] = []\n",
    "    for person in people:\n",
    "        for activity in activities:\n",
    "            curr_df = None\n",
    "            for sensor in sensors:\n",
    "                curr_df = person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True) if curr_df is None else curr_df.merge(\n",
    "                    person[sensor].query(f\"`ACTIVITY`=='{activity}'\").drop(columns=['class','ACTIVITY']).add_suffix(sensor).reset_index(drop=True), left_index=True, right_index=True)\n",
    "            curr_df['ACTIVITY'] = activity\n",
    "            people_df.append(curr_df)\n",
    "    return pd.concat(people_df).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_dropped = []\n",
    "for n in ['X', 'Y', 'Z']:\n",
    "    columns_to_be_dropped.append(f'\"{n}VAR\"')\n",
    "    for i in range(13):\n",
    "        columns_to_be_dropped.append(f'\"{n}MFCC{i}\"')\n",
    "for n in combinations(['X', 'Y', 'Z'], 2):\n",
    "    columns_to_be_dropped.append(f'\"{\"\".join(n)}COS\"')\n",
    "    columns_to_be_dropped.append(f'\"{\"\".join(n)}COR\"')\n",
    "\n",
    "sensors = [\"phone_accel\", \"watch_accel\", \"phone_gyro\", \"watch_gyro\"]\n",
    "activities = {'A': 'walking',\n",
    "              'B': 'jogging',\n",
    "              'C': 'stairs',\n",
    "              'D': 'sitting',\n",
    "              'E': 'standing',\n",
    "              'F': 'typing',\n",
    "              'G': 'teeth',\n",
    "              'H': 'soup',\n",
    "              'I': 'chips',\n",
    "              'J': 'pasta',\n",
    "              'K': 'drinking',\n",
    "              'L': 'sandwich',\n",
    "              'M': 'kicking',\n",
    "              'O': 'catch',\n",
    "              'P': 'dribbling',\n",
    "              'Q': 'writing',\n",
    "              'R': 'clapping',\n",
    "              'S': 'folding'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "people: list[dict[str, pd.DataFrame]] = [{\n",
    "    'guid': i,\n",
    "    'phone_accel': processed_dataframe(pd.DataFrame(arff.loadarff(f\"arff_files/phone/accel/data_{i}_accel_phone.arff\")[0]), columns_to_be_dropped),\n",
    "    'phone_gyro': processed_dataframe(pd.DataFrame(arff.loadarff(f\"arff_files/phone/gyro/data_{i}_gyro_phone.arff\")[0]), columns_to_be_dropped),\n",
    "    'watch_accel': processed_dataframe(pd.DataFrame(arff.loadarff(f\"arff_files/watch/accel/data_{i}_accel_watch.arff\")[0]), columns_to_be_dropped),\n",
    "    'watch_gyro': processed_dataframe(pd.DataFrame(arff.loadarff(f\"arff_files/watch/gyro/data_{i}_gyro_watch.arff\")[0]), columns_to_be_dropped)\n",
    "} for i in range(1600, 1651) if i != 1614]  # person 1614 missing in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_combination = [[i]for i in sensors] + \\\n",
    "    [list(i) for i in list(combinations(sensors, 2))]+[sensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "biometry_models = {activity: [] for activity in activities}\n",
    "for person in people:\n",
    "    impostors = pick_impostors(person['guid'], people)\n",
    "    for activity in activities:\n",
    "        train, test = biometric_train_test_split(\n",
    "            person, impostors, activity, sensors)\n",
    "        if len(train) == 0 or len(test)==0:\n",
    "            continue\n",
    "        train_target = train['class']\n",
    "        # print(train_target.unique())\n",
    "        train = train.drop(columns=['class'])\n",
    "        test_target = test['class']\n",
    "        test = test.drop(columns=['class'])\n",
    "        biometry_classifier = RandomForestClassifier(\n",
    "            10, max_features='sqrt')\n",
    "        biometry_classifier.fit(train, train_target)\n",
    "        y_true = test_target\n",
    "        if len(y_true.unique())!=2:\n",
    "            # print(person['guid'],combination,activity)\n",
    "            continue\n",
    "        eer = compute_eer_changjiang(test_target,biometry_classifier.predict(test))\n",
    "        biometry_models[activity].append(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking 0.12244160623842794\n",
      "jogging 0.13106548407771806\n",
      "stairs 0.18339670716198295\n",
      "sitting 0.1657020531780547\n",
      "standing 0.23001185626514298\n",
      "typing 0.15048178626732367\n",
      "teeth 0.18161459820042322\n",
      "soup 0.17670430912922583\n",
      "chips 0.16118067718137524\n",
      "pasta 0.20546018099912405\n",
      "drinking 0.17555747405992694\n",
      "sandwich 0.1766961822210309\n",
      "kicking 0.22811693257414256\n",
      "catch 0.16451797455898662\n",
      "dribbling 0.1878989312509228\n",
      "writing 0.1643320248292602\n",
      "clapping 0.15199607373286309\n",
      "folding 0.18330402713449842\n"
     ]
    }
   ],
   "source": [
    "for activity in biometry_models:\n",
    "    biometry_models[activity] = list(filter(lambda x: x!=0,biometry_models[activity]))\n",
    "    print(activities[activity], np.average(biometry_models[activity]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walking': 0.9814621758887997,\n",
       " 'jogging': 0.9446939321037157,\n",
       " 'stairs': 0.9270588235294118,\n",
       " 'sitting': 0.925921855921856,\n",
       " 'standing': 0.8939479060265578,\n",
       " 'typing': 0.9733360064153969,\n",
       " 'teeth': 0.9532915360501567,\n",
       " 'soup': 0.9528735632183908,\n",
       " 'chips': 0.9546220633299285,\n",
       " 'pasta': 0.9592202462380301,\n",
       " 'drinking': 0.9368253968253969,\n",
       " 'sandwich': 0.9620820271682341,\n",
       " 'kicking': 0.9079775280898877,\n",
       " 'catch': 0.9430232558139535,\n",
       " 'dribbling': 0.947114402451481,\n",
       " 'writing': 0.9418364681295716,\n",
       " 'clapping': 0.9564794007490637,\n",
       " 'folding': 0.933283395755306}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "scores: dict[str,float] = {}\n",
    "for activity in activities:\n",
    "    people_df = person_classification_df_builder(people,sensors,activity)\n",
    "    people_target = people_df['class']\n",
    "    people_df = people_df.drop(columns=['class'])\n",
    "    people_classifier = RandomForestClassifier(10, max_features='sqrt')\n",
    "    score = cross_validate(people_classifier,people_df,people_target,cv=kf,scoring='accuracy')\n",
    "    scores[activities[activity]]= np.average(score['test_score'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10,random_state=1, shuffle=True)\n",
    "activities_df = activity_classification_df_builder(people, sensors,activities)\n",
    "activities_target = activities_df['ACTIVITY']\n",
    "activities_df = activities_df.drop(columns=['ACTIVITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier()\n",
    "# optimize_rf_params = {'criterion':['gini','entropy','log_loss'], 'max_features':['sqrt','log2',None], 'n_estimators':(1,50)}\n",
    "# rkf = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=1)\n",
    "# random_search = RandomizedSearchCV(estimator=classifier,param_distributions=optimize_rf_params,n_jobs=-1,scoring='accuracy',cv=rkf)\n",
    "# random_search.fit(activities_df,activities_target)\n",
    "# best_model = random_search.best_estimator_\n",
    "# print(random_search.best_score_)\n",
    "# print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8950853889943076"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_rfc = RandomForestClassifier(50,max_features='log2')\n",
    "score_rfc = cross_validate(activity_rfc, activities_df,activities_target,scoring='accuracy',cv=kf)\n",
    "activity_accuracy = np.average(score_rfc['test_score'])\n",
    "activity_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
